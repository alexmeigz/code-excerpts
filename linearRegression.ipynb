{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS 165B HW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHvsq_1w_blJ"
      },
      "source": [
        "Alex Mei \\\\\n",
        "CS 165B \\\\\n",
        "Fall, 2021 \\\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHixIXHP-pZF"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import numpy.linalg as npla\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B2eFDhsCLGH"
      },
      "source": [
        "# Generate Data along the line y = x1 with noise in range(-0.5, 0.5)\n",
        "# x2, ..., xn are all noise featurs in range (-0.5, 0.5)\n",
        "# x0 is valued \"1\" for the w0 weight bias\n",
        "def generateRegressionData(sampleSize: int, features: int):\n",
        "  # 2 element tuple containing\n",
        "  # param \"sampleSize\" lengthed list of feature rows containing [1, value in range(999.5, 1000.5), param \"features\" - 1 values in range(-0.5, 0.5)]\n",
        "  # param\"sampleSize\" lengthed  list of labels with range (999.5, 1000.5) \n",
        "  return (\n",
        "           [[1] + [10 + i + (random.random() - 0.5)] + [(random.random() - 0.5) for j in range(features - 1)] for i in range(sampleSize)],\n",
        "           [10 + i + (random.random() - 0.5) for i in range(sampleSize)]\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PJ0_V9WD-07"
      },
      "source": [
        "# Note: when regularization term = 0 (default), this is Least Squares Regression. Otherwise, it is Ridge Regression. \n",
        "def errorFunction(weights: np.array, data: np.array, labels: np.array, regularization: float = 0):\n",
        "  return 0.5 * (npla.norm(data @ weights - labels, 2) ** 2 + regularization * npla.norm(weights) ** 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEv3HTzhCfBk"
      },
      "source": [
        "# Note: when regularization term = 0 (default), this is Least Squares Regression. Otherwise, it is Ridge Regression. \n",
        "# Note: solution not guaranteed to exist when regularization term is zero\n",
        "def closedFormSolver(data: np.array, labels: np.array, regularization: float = 0):\n",
        "  return npla.inv(data.T @ data + regularization * np.identity(np.shape(data)[1])) @ data.T @ labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc-h_gNqAUP_"
      },
      "source": [
        "def gradientDescentStep(\n",
        "    weights: np.array, \n",
        "    data: np.array, \n",
        "    labels: np.array, \n",
        "    stepSize: float, \n",
        "    batchSize: int = 0, \n",
        "    regularization: float = 0):\n",
        "  datasetSize = np.shape(data)[0]\n",
        "\n",
        "  # invalid batchSize, use batchSize = datasetSize\n",
        "  if batchSize < 1 or batchSize >= datasetSize:\n",
        "    selectedData = data\n",
        "    selectedLabels = labels\n",
        "\n",
        "  # else, randomly select batch of data given batch size\n",
        "  else:\n",
        "    choices = np.random.choice(datasetSize, size=batchSize, replace=False)\n",
        "    selectedData = data[choices][:]\n",
        "    selectedLabels = labels[choices]\n",
        "\n",
        "  # compute gradient and direction vector\n",
        "  gradient = selectedData.T @ selectedData @ weights - selectedData.T @ selectedLabels - regularization * weights\n",
        "  direction = -1 * gradient / npla.norm(gradient, 2)\n",
        "\n",
        "  # return update weights\n",
        "  return weights + stepSize * direction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx0NO0mozKs7"
      },
      "source": [
        "def gradientDescent(\n",
        "    weights: np.array, \n",
        "    data: np.array, \n",
        "    labels: np.array, \n",
        "    stepSize: float, \n",
        "    iterations: int,\n",
        "    granularity: int, \n",
        "    plot: bool = False,\n",
        "    graphTitle: str = \"\",\n",
        "    batchSize: int = 0, \n",
        "    regularization: float = 0):\n",
        "  \n",
        "  plottingIterationNumber = list()\n",
        "  plottingError = list()\n",
        "\n",
        "  for i in range(1, iterations + 2):\n",
        "    # reporting based on granularity \n",
        "    if i % granularity == 1:\n",
        "      plottingIterationNumber.append(i)\n",
        "      # plottingError.append(errorFunction(weights, data, labels, regularization))\n",
        "      # plot the log error instead to be able to see a difference in the graphs\n",
        "      plottingError.append(np.log(errorFunction(weights, data, labels, regularization)))\n",
        "\n",
        "    # update weights\n",
        "    weights = gradientDescentStep(weights, data, labels, stepSize, batchSize=batchSize, regularization=regularization)\n",
        "\n",
        "  # convergence graph plot\n",
        "  if plot:\n",
        "    plt.plot(plottingIterationNumber, plottingError)\n",
        "\n",
        "    # Plot configs\n",
        "    plt.xlabel(\"Iteration Number\")\n",
        "    plt.ylabel(\"Objective Function\")\n",
        "    plt.title(\"Convergence Graph for {}\".format(graphTitle))\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "  return weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20K9PjnjDwsk"
      },
      "source": [
        "def normDistance(v1, v2):\n",
        "  return npla.norm(v1 - v2, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra-3L_VAS91G"
      },
      "source": [
        "def mse(predicted: np.array, actual: np.array):\n",
        "  return npla.norm(predicted - actual, 2) ** 2 / np.shape(predicted)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-vMOsm7DsNX"
      },
      "source": [
        "# Q4. L2 Differences\n",
        "\n",
        "# generate random data\n",
        "data, labels = generateRegressionData(sampleSize=1000, features=50)\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "initialWeights = np.random.random(np.shape(data)[1])\n",
        "\n",
        "# hyperparameters\n",
        "regularization = 10 ** -6   # regularization term\n",
        "stepSize = 10 ** -4         # learning rate\n",
        "iterations = 10 ** 4        # maximum iterations\n",
        "granularity = 10 ** 4       # plot reporting datapoint granularity\n",
        "\n",
        "### Least Squares Regression\n",
        "# Closed Form Solution\n",
        "optimalWeights = closedFormSolver(data, labels)\n",
        "\n",
        "# Gradient Descent\n",
        "weights = initialWeights.copy()\n",
        "gdWeights = gradientDescent(weights, data, labels, stepSize, iterations, granularity)\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "weights = initialWeights.copy()\n",
        "sgdWeights = gradientDescent(weights, data, labels, stepSize, iterations, granularity, batchSize=1)\n",
        "\n",
        "print(\"{} Least Squares Regression {}\".format(\"-\" * 10, \"-\" * 10))\n",
        "print(\"L2 Norm Difference of Weights between Gradient Descent & Stochastic Gradient Descent: {}\".format(normDistance(gdWeights, sgdWeights)))\n",
        "print(\"L2 Norm Difference of Weights between Gradient Descent & Closed Form Solution: {}\".format(normDistance(gdWeights, optimalWeights)))\n",
        "print(\"L2 Norm Difference of Weights between Stochastic Gradient Descent & Closed Form Solution: {}\".format(normDistance(sgdWeights, optimalWeights)))\n",
        "\n",
        "### Ridge Regression\n",
        "# Closed Form Solution\n",
        "optimalWeights = closedFormSolver(data, labels, regularization)\n",
        "\n",
        "# Gradient Descent\n",
        "weights = initialWeights.copy()\n",
        "gdWeights = gradientDescent(weights, data, labels, stepSize, iterations, granularity, regularization=regularization)\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "weights = initialWeights.copy()\n",
        "sgdWeights = gradientDescent(weights, data, labels, stepSize, iterations, granularity, regularization=regularization, batchSize=1)\n",
        "\n",
        "print(\"{} Ridge Regression {}\".format(\"-\" * 10, \"-\" * 10))\n",
        "print(\"L2 Norm Difference of Weights between Gradient Descent & Stochastic Gradient Descent: {}\".format(normDistance(gdWeights, sgdWeights)))\n",
        "print(\"L2 Norm Difference of Weights between Gradient Descent & Closed Form Solution: {}\".format(normDistance(gdWeights, optimalWeights)))\n",
        "print(\"L2 Norm Difference of Weights between Stochastic Gradient Descent & Closed Form Solution: {}\".format(normDistance(sgdWeights, optimalWeights)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMcZ5Yds2eEP"
      },
      "source": [
        "# Q5. Convergence Graphs\n",
        "\n",
        "# generate random data\n",
        "data, labels = generateRegressionData(sampleSize=1000, features=50)\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "initialWeights = np.random.random(np.shape(data)[1])\n",
        "\n",
        "# hyperparameters\n",
        "regularization = 10 ** -6   # regularization term\n",
        "stepSize = 10 ** -4         # learning rate\n",
        "iterations = 10 ** 4        # maximum iterations\n",
        "granularity = 100           # number of iterations for early stopping consideration\n",
        "\n",
        "# Least Squares Regression Gradient Descent\n",
        "weights = initialWeights.copy()\n",
        "weights = gradientDescent(weights, data, labels, stepSize, iterations, granularity, plot=True, graphTitle=\"Least Squares Regression Gradient Descent\\n\")\n",
        "print(weights[0])\n",
        "\n",
        "# Least Squares Regression Stochastic Gradient Descent\n",
        "weights = initialWeights.copy()\n",
        "weights = gradientDescent(weights, data, labels, stepSize, iterations, granularity, batchSize=1, plot=True, graphTitle=\"Least Squares Regression Stochastic Gradient Descent\\n\")\n",
        "print(weights[0])\n",
        "\n",
        "# Ridge Regression Gradient Descent\n",
        "weights = initialWeights.copy()\n",
        "weights = gradientDescent(weights, data, labels, stepSize, iterations, granularity, regularization=regularization, plot=True, graphTitle=\"Ridge Regression Gradient Descent\\n\")\n",
        "print(weights[0])\n",
        "\n",
        "# Ridge Regression Stochastic Gradient Descent\n",
        "weights = initialWeights.copy()\n",
        "weights = gradientDescent(weights, data, labels, stepSize, iterations, granularity, regularization=regularization, batchSize=1, plot=True, graphTitle=\"Ridge Regression Stochastic Gradient Descent\\n\")\n",
        "print(weights[0])\n",
        "\n",
        "# Printing weights[0] of each vector manually confirm they aren't exactly identical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdQWyyNFFtzW"
      },
      "source": [
        "# Q6. Batched Gradient Descent\n",
        "\n",
        "# generate random data\n",
        "data, labels = generateRegressionData(sampleSize=5000, features=1000)\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "initialWeights = np.random.random(np.shape(data)[1])\n",
        "\n",
        "# hyperparameters\n",
        "regularization = 10 ** -6   # regularization term\n",
        "stepSize = 10 ** -4         # learning rate\n",
        "iterations = 10 ** 4        # maximum iterations\n",
        "granularity = 100           # number of iterations for early stopping consideration\n",
        "\n",
        "# Ridge Regression Gradient Descent Batch Size = 1\n",
        "weights = initialWeights.copy()\n",
        "weights = gradientDescent(weights, data, labels, stepSize, iterations, granularity, regularization=regularization, batchSize=1, plot=True, graphTitle=\"Ridge Regression Gradient Descent Batch Size = 1\\n\")\n",
        "print(weights[0])\n",
        "\n",
        "# Ridge Regression Gradient Descent Batch Size = 10\n",
        "weights = initialWeights.copy()\n",
        "weights = gradientDescent(weights, data, labels, stepSize, iterations, granularity, regularization=regularization, batchSize=10, plot=True, graphTitle=\"Ridge Regression Gradient Descent Batch Size = 10\\n\")\n",
        "print(weights[0])\n",
        "\n",
        "\n",
        "# Ridge Regression Gradient Descent Batch Size = 100\n",
        "weights = initialWeights.copy()\n",
        "weights = gradientDescent(weights, data, labels, stepSize, iterations, granularity, regularization=regularization, batchSize=100, plot=True, graphTitle=\"Ridge Regression Gradient Descent Batch Size = 100\\n\")\n",
        "print(weights[0])\n",
        "\n",
        "# Printing weights[0] of each vector manually confirm they aren't exactly identical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X61DsXCDNfGE"
      },
      "source": [
        "# Q7a. Generalizion Performance with Increasing Sample Size\n",
        "\n",
        "# generate random data\n",
        "data, labels = generateRegressionData(sampleSize=1000, features=100)\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# train - test split\n",
        "trainingData = data[:900]\n",
        "trainingLabels = data[:900]\n",
        "testingData = data[900:]\n",
        "testingLabels = data[900:]\n",
        "\n",
        "# hyperparameters\n",
        "regularization = 10 ** -6   # regularization term\n",
        "\n",
        "# trainingSize vs MSE\n",
        "trainingSize = list()\n",
        "meanSquaredError = list()\n",
        "\n",
        "for i in range(50, 901, 50):\n",
        "  # Compute Closed Form Solution\n",
        "  optimalWeights = closedFormSolver(trainingData[:i], trainingLabels[:i], regularization)\n",
        "\n",
        "  # Compute MSE\n",
        "  trainingSize.append(i)\n",
        "  meanSquaredError.append(mse(testingData @ optimalWeights, testingLabels))\n",
        "\n",
        "# convergence graph plot\n",
        "plt.plot(trainingSize, meanSquaredError)\n",
        "\n",
        "# Plot configs\n",
        "plt.xlabel(\"Training Sample Size\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.title(\"Generalizion Performance with Increasing Sample Size\")\n",
        "    \n",
        "plt.show()\n",
        "\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brKLdIRgT6-g"
      },
      "source": [
        "# Q7b. Generalizion Performance with Increasing Dimensions\n",
        "\n",
        "# generate random data\n",
        "data, labels = generateRegressionData(sampleSize=1000, features=450)\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# train - test split\n",
        "trainingData = data[:900]\n",
        "trainingLabels = data[:900]\n",
        "testingData = data[900:]\n",
        "testingLabels = data[900:]\n",
        "\n",
        "# hyperparameters\n",
        "regularization = 10 ** -6   # regularization term\n",
        "\n",
        "# featureSize vs MSE\n",
        "featureSize = list()\n",
        "meanSquaredError = list()\n",
        "\n",
        "for i in range(100, 451, 50):\n",
        "  # Compute Closed Form Solution\n",
        "  optimalWeights = closedFormSolver(trainingData[:,:i], trainingLabels, regularization)\n",
        "\n",
        "  # Compute MSE\n",
        "  featureSize.append(i)\n",
        "  meanSquaredError.append(mse(testingData[:,:i] @ optimalWeights, testingLabels))\n",
        "\n",
        "# convergence graph plot\n",
        "plt.plot(featureSize, meanSquaredError)\n",
        "\n",
        "# Plot configs\n",
        "plt.xlabel(\"Feature Size\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.title(\"Generalizion Performance with Increasing Dimensions\")\n",
        "    \n",
        "plt.show()\n",
        "\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upwfe68sU_eD"
      },
      "source": [
        "# Q8. Computational Complexity of Closed Form Solution\n",
        "\n",
        "# generate random data\n",
        "data, labels = generateRegressionData(sampleSize=1000, features=2000)\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# hyperparameters\n",
        "regularization = 10 ** -6   # regularization term\n",
        "\n",
        "# featureSize vs MSE\n",
        "featureSize = list()\n",
        "computationTime = list()\n",
        "\n",
        "for i in range(100, 2001, 100):\n",
        "  t1 = time.time()\n",
        "\n",
        "  # Compute Closed Form Solution\n",
        "  optimalWeights = closedFormSolver(data[:,:i], labels, regularization)\n",
        "  t2 = time.time()\n",
        "\n",
        "  computationTime.append(t2 - t1)\n",
        "  featureSize.append(i)\n",
        "\n",
        "# convergence graph plot\n",
        "plt.plot(featureSize, computationTime)\n",
        "\n",
        "# Plot configs\n",
        "plt.xlabel(\"Feature Size\")\n",
        "plt.ylabel(\"Computation Time\")\n",
        "plt.title(\"Computational Complexity of Closed Form Solution\")\n",
        "    \n",
        "plt.show()\n",
        "\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}